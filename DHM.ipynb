{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvise from https://www.datacamp.com/community/tutorials/autoencoder-keras-tutorial\n",
    "for Deep Hybrid Network with GAN\n",
    "\n",
    "Good rpactice:\n",
    "1. keras_inception = pandas, flow from pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras_gan_rgb import GAN\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import gzip\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dense, Flatten, Dropout, Concatenate, concatenate\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, BaseWrapper\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(28 * 28 * num_images)\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        data = data.reshape(num_images, 28,28)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_labels(filename, num_images):\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = extract_data('data_notMNIST-to-MNIST/train-images-idx3-ubyte.gz', 60000)\n",
    "test_data = extract_data('data_notMNIST-to-MNIST/t10k-images-idx3-ubyte.gz', 10000)\n",
    "\n",
    "train_labels = extract_labels('data_notMNIST-to-MNIST/train-labels-idx1-ubyte.gz',60000)\n",
    "test_labels = extract_labels('data_notMNIST-to-MNIST/t10k-labels-idx1-ubyte.gz',10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of target classes\n",
    "label_dict = {\n",
    " 0: 'A',\n",
    " 1: 'B',\n",
    " 2: 'C',\n",
    " 3: 'D',\n",
    " 4: 'E',\n",
    " 5: 'F',\n",
    " 6: 'G',\n",
    " 7: 'H',\n",
    " 8: 'I',\n",
    " 9: 'J',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# TODO: random plot\n",
    "plt.figure(figsize=[5,5])\n",
    "\n",
    "# Display the first image in training data\n",
    "plt.subplot(121)\n",
    "curr_img = np.reshape(train_data[9], (28,28))\n",
    "curr_lbl = train_labels[0]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\n",
    "# Display the first image in testing data\n",
    "plt.subplot(122)\n",
    "curr_img = np.reshape(test_data[9], (28,28))\n",
    "curr_lbl = test_labels[0]\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original train_data = (None, 28, 28) --> channel last\n",
    "train_data = train_data.reshape(-1, 28,28, 1)\n",
    "test_data = test_data.reshape(-1, 28,28, 1)\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data / np.max(train_data)\n",
    "test_data = test_data / np.max(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D : (train_data, train_data, train_labels) -> (train_X,valid_X,train_labels,valid_labels)\n",
    "train_X,valid_X, train_ground, valid_ground, train_labels, valid_labels = train_test_split(train_data, \n",
    "                                                                                          train_data, \n",
    "                                                                                          train_labels, \n",
    "                                                                                          test_size=0.2,\n",
    "                                                                                          random_state=13,\n",
    "                                                                                          stratify=train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "# inChannel = 1\n",
    "# x, y = 28, 28\n",
    "\n",
    "input_shape = train_X[0,0:].shape\n",
    "losses = {\"D_out\": \"categorical_crossentropy\", \"G_out\": \"mse\"}\n",
    "alpha = [10**0, 10**1, 10**2, 10**3, 10**4, 10**5]\n",
    "beta  = [1., 2., 3., 4., 5.]\n",
    "loss_weights = {\"D_out\": alpha, \"G_out\": beta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameterised model (function/class)\n",
    "model_name = \"dhm_autoencoder\"\n",
    "\n",
    "def dhm_autoencoder(opt='sgd',input_shape=input_shape, losses=losses, plotModel=True):\n",
    "    # construct, compile and return a Keras model\n",
    "    \n",
    "    # ==================\n",
    "    # autoencoder\n",
    "    # ==================\n",
    "    #encoder\n",
    "    #input = 28 x 28 x 1 (wide and thin)\n",
    "    input_img = Input(shape =input_shape, name=\"Input\")\n",
    "    #\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "    #decoder\n",
    "    conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
    "    up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
    "    conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
    "    up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
    "    G_out = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name=\"G_out\")(up2) # 28 x 28 x 1\n",
    "    #\n",
    "    # ==================\n",
    "    # discriminator\n",
    "    # ==================\n",
    "    conv1_d  = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "    pool1_d  = MaxPooling2D(pool_size=(2, 2))(conv1_d) #14 x 14 x 32\n",
    "    conv2_d  = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1_d) #14 x 14 x 64\n",
    "    pool2_d  = MaxPooling2D(pool_size=(2, 2))(conv2_d) #7 x 7 x 64\n",
    "    conv3_d  = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2_d) #7 x 7 x 128 (small and thick)\n",
    "    #\n",
    "    merge    = concatenate([conv3_d, conv3])\n",
    "    pool3_d  = MaxPooling2D(pool_size=(2, 2))(merge)\n",
    "    dropout1 = Dropout(0.25)(pool3_d)\n",
    "    #\n",
    "    flatten  = Flatten()(pool3_d)\n",
    "    dense    = Dense(128, activation='relu')(flatten)\n",
    "    dropout2 = Dropout(0.5)(dense)\n",
    "    D_out    = Dense(10, activation='softmax', name=\"D_out\")(dropout2)\n",
    "    #\n",
    "    \n",
    "    model    = Model(inputs=[input_img], outputs=[D_out, G_out])\n",
    "    model    = model.compile(optimizer=opt, losses=losses , metrics='accuracy',)\n",
    "    #\n",
    "    if plotModel == True:\n",
    "        plot_model(model, to_file=model_name+\".png\", show_shapes=True)\n",
    "    #\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasClassifier_ext(BaseWrapper):\n",
    "    \"\"\"Implementation of the scikit-learn classifier API for Keras.\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, x, y, sample_weight=None, **kwargs):\n",
    "        y = np.array(y)\n",
    "        if len(y.shape) == 2 and y.shape[1] > 1:\n",
    "            self.classes_ = np.arange(y.shape[1])\n",
    "        elif (len(y.shape) == 2 and y.shape[1] == 1) or len(y.shape) == 1:\n",
    "            self.classes_ = np.unique(y)\n",
    "            y = np.searchsorted(self.classes_, y)\n",
    "        else:\n",
    "            raise ValueError('Invalid shape for y: ' + str(y.shape))\n",
    "        self.n_classes_ = len(self.classes_)\n",
    "        if sample_weight is not None:\n",
    "            kwargs['sample_weight'] = sample_weight\n",
    "        return super(KerasClassifier_ext, self).fit(x, y, **kwargs)\n",
    "\n",
    "    def predict(self, x, **kwargs):\n",
    "        kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n",
    "\n",
    "        proba = self.model.predict(x, **kwargs)\n",
    "        if proba.shape[-1] > 1:\n",
    "            classes = proba.argmax(axis=-1)\n",
    "        else:\n",
    "            classes = (proba > 0.5).astype('int32')\n",
    "        return self.classes_[classes]\n",
    "\n",
    "    def predict_proba(self, x, **kwargs):\n",
    "        kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n",
    "        probs = self.model.predict(x, **kwargs)\n",
    "\n",
    "        # check if binary classification\n",
    "        if probs.shape[1] == 1:\n",
    "            # first column is probability of class 0 and second is of class 1\n",
    "            probs = np.hstack([1 - probs, probs])\n",
    "        return probs\n",
    "\n",
    "    def score(self, x, y, **kwargs):\n",
    "        y = np.searchsorted(self.classes_, y)\n",
    "        kwargs = self.filter_sk_params(Sequential.evaluate, kwargs)\n",
    "\n",
    "        loss_name = self.model.loss\n",
    "        if hasattr(loss_name, '__name__'):\n",
    "            loss_name = loss_name.__name__\n",
    "        if loss_name == 'categorical_crossentropy' and len(y.shape) != 2:\n",
    "            y = to_categorical(y)\n",
    "\n",
    "        outputs = self.model.evaluate(x, y, **kwargs)\n",
    "        outputs = to_list(outputs)\n",
    "        for name, output in zip(self.model.metrics_names, outputs):\n",
    "            if name == 'acc':\n",
    "                return output\n",
    "        raise ValueError('The model is not configured to compute accuracy. '\n",
    "                         'You should pass `metrics=[\"accuracy\"]` to '\n",
    "                         'the `model.compile()` method.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple output layers in Scikit-Learn wrappers\n",
    "# Source : https://github.com/keras-team/keras/issues/9001\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class KerasModel(BaseEstimator):\n",
    "    def __init__(self, optimizer = 'sgd', losses = losses, plotModel = True):\n",
    "        self.optimizer = optimizer # an example of a tunable hyperparam\n",
    "        self.losses = losses\n",
    "        self.plotModel = True\n",
    "    # in GridSearchCV, expect 'fit' and 'predict' method\n",
    "    # use BaseExtimator to generalize \n",
    "    def fit(self, X, y):  # 'fit' means, 'fit' a 'KerasModel'\n",
    "        #\n",
    "        # from original code; X is Dictionary\n",
    "        # make 'fit' receive dictionanry of 'y'\n",
    "        input_img = Input(shape=input_shape, name=\"Input\")\n",
    "        #\n",
    "        conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
    "        conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
    "        conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
    "        #decoder\n",
    "        conv4 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 128\n",
    "        up1 = UpSampling2D((2,2))(conv4) # 14 x 14 x 128\n",
    "        conv5 = Conv2D(64, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 64\n",
    "        up2 = UpSampling2D((2,2))(conv5) # 28 x 28 x 64\n",
    "        G_out = Conv2D(1, (3, 3), activation='sigmoid', padding='same', name=\"G_out\")(up2) # 28 x 28 x 1\n",
    "        # ==================\n",
    "        # discriminator\n",
    "        # ==================\n",
    "        conv1_d  = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
    "        pool1_d  = MaxPooling2D(pool_size=(2, 2))(conv1_d) #14 x 14 x 32\n",
    "        conv2_d  = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1_d) #14 x 14 x 64\n",
    "        pool2_d  = MaxPooling2D(pool_size=(2, 2))(conv2_d) #7 x 7 x 64\n",
    "        conv3_d  = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2_d) #7 x 7 x 128 (small and thick)\n",
    "        #\n",
    "        merge    = concatenate([conv3_d, conv3])\n",
    "        pool3_d  = MaxPooling2D(pool_size=(2, 2))(merge)\n",
    "        dropout1 = Dropout(0.25)(pool3_d)\n",
    "        #\n",
    "        flatten  = Flatten()(pool3_d)\n",
    "        dense    = Dense(128, activation='relu')(flatten)\n",
    "        dropout2 = Dropout(0.5)(dense)\n",
    "        D_out    = Dense(10, activation='softmax', name=\"D_out\")(dropout2)\n",
    "        #\n",
    "        if self.plotModel == True:\n",
    "            plot_model(model, to_file=model_name+\".png\", show_shapes=True)  # make it into self\n",
    "        ###\n",
    "        self.model = Model(inputs = input_img, outputs = [D_out, G_out])\n",
    "        self.model.compile(self.optimizer, 'mse', losses=losses, metrics='accuracy')\n",
    "        # call fit model\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "dhm = make_pipeline(KerasModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build \n",
    "# dhm = dhm_autoencoder(x, y, inChannel, opt, losses, alpha, beta, plot_model=True)\n",
    "# model = KerasClassifier(build_fn=create_model, epochs=100, batch_size=10, verbose=0)\n",
    "# dhm_autoencoder() / KerasModel()\n",
    "dhm = KerasClassifier(build_fn=KerasModel, \n",
    "                      batch_size=batch_size, \n",
    "                      epochs=epochs, \n",
    "                      verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gridsearch for single I/O\n",
    "grid = GridSearchCV(estimator=dhm, param_grid=loss_weights, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X.shape, train_labels.reshape((train_labels.shape[0], 1)).shape, train_ground.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [48000, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e9dbc96b56a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train_y = {\"D_out\": to_categorical(train_labels.reshape((train_labels.shape[0], 1)).shape)}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0;31m# Regenerate parameter iterable for each fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [48000, 2]"
     ]
    }
   ],
   "source": [
    "train_y = {\"D_out\": to_categorical(train_labels.reshape((train_labels.shape[0], 1)).shape), \"G_out\" : train_ground}\n",
    "# train_y = {\"D_out\": to_categorical(train_labels.reshape((train_labels.shape[0], 1)).shape)}\n",
    "\n",
    "grid_result = grid.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit-learn pipeline\n",
    "dhm_train = dhm.fit(train_X, {\"D_out\": to_categorical(train_labels), \"G_out\" : train_ground}, \n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(valid_X, {\"D_out\": to_categorical(valid_labels), \"G_out\" : valid_ground}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = dhm_train.history['D_out_loss']\n",
    "val_loss = dhm_train.history['val_loss']\n",
    "\n",
    "np.savetxt(model_name+\"_D_out_loss.txt\", np.array(loss), delimiter=\",\")\n",
    "np.savetxt(model_name+\"_D_out_val_loss.txt\", np.array(val_loss), delimiter=\",\")\n",
    "\n",
    "epochs_ = range(epochs)\n",
    "plt.figure()\n",
    "plt.plot(epochs_, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs_, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = dhm_train.history['G_out_loss']\n",
    "val_loss = dhm_train.history['val_loss']\n",
    "\n",
    "np.savetxt(model_name+\"_G_out_loss.txt\", np.array(loss), delimiter=\",\")\n",
    "np.savetxt(model_name+\"_G_out_val_loss.txt\", np.array(val_loss), delimiter=\",\")\n",
    "\n",
    "epochs_ = range(epochs)\n",
    "plt.figure()\n",
    "plt.plot(epochs_, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs_, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pred_D, pred_G] = dhm.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_D.shape, pred_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_D analysis: confsion matrix, top-2 accuracy etc.\n",
    "# use sklearn.metrics.classification_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_G analysis:\n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Test Images\")\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(test_data[i, ..., 0], cmap='gray')\n",
    "    curr_lbl = test_labels[i]\n",
    "    plt.title(\"(Label: \" + str(label_dict[curr_lbl]) + \")\")\n",
    "plt.show()    \n",
    "plt.figure(figsize=(20, 4))\n",
    "print(\"Reconstruction of Test Images\")\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 10, i+1)\n",
    "    plt.imshow(pred_G[i, ..., 0], cmap='gray')  \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
